# ğŸ§  WinCLIP Prompt Engineering Analysis

A graduate research project exploring **zero-/few-shot anomaly classification and segmentation** using **CLIP (Contrastive Languageâ€“Image Pretraining)** and prompt engineeringâ€”without the need for any model retraining. This work builds on the **WinCLIP** and **WinCLIP+** frameworks, aiming to enable scalable and instantly deployable defect detection solutions in industrial settings.

## ğŸš€ Overview

Traditional anomaly detection models rely on large datasets of normal images and intensive retraining. In contrast, **WinCLIP** leverages natural language prompts to detect visual defects using pre-trained vision-language modelsâ€”allowing **zero-shot** classification and **pixel-level segmentation**.

This project:
- Reconstructs and extends the original WinCLIP architecture.
- Implements **prompt engineering strategies** to boost zero-shot accuracy.
- Combines **ViT-B/32** and **ConvNeXt** models for a hybrid ensemble approach.
- Adapts CLIP to grayscale datasets like **Fashion-MNIST**, simulating domain shifts in real-world applications.

## ğŸ§© Key Features

- ğŸ” **Language-Guided Defect Detection**: Detect "normal" vs. "anomalous" states using compositional natural-language prompts.
- ğŸ–¼ï¸ **Zero-Shot Segmentation**: Extracts multi-scale features with CLIP to localize anomalies at the pixel level.
- ğŸ§  **Few-Shot Enhancement (WinCLIP+)**: Improves performance using just 1â€“4 reference images of "normal" objects.
- ğŸ§ª **Prompt Engineering**: Evaluated multiple prompt variants for domain adaptation on Fashion-MNIST.
- âš™ï¸ **Model Ensembling**: Fused CLIP models (ViT-B/32 + ConvNeXt-Base) to improve classification robustness.

## ğŸ“Š Results

| Model                  | Top-1 Accuracy | Top-5 Accuracy |
|------------------------|----------------|----------------|
| ViT-B/32 (CLIP)        | 72.99%         | 99.46%         |
| ConvNeXt-Base (CLIP)   | 73.45%         | 99.26%         |
| **Ensemble (Ours)**    | **76.15%**     | **99.60%**     |

> Achieved **91.8% AUROC** for zero-shot classification and **95.2% pixel-AUROC** with few-shot segmentationâ€”without task-specific training.

## ğŸ› ï¸ Tech Stack

- Python Â· PyTorch  
- CLIP Â· OpenCLIP Â· Vision Transformers (ViT-B/32)  
- ConvNeXt  
- Fashion-MNIST Dataset


## ğŸ“Œ Future Work

- ğŸ”¬ Explore larger backbones (e.g., ViT-L/14, ConvNeXt-Large)
- ğŸ§  Add confidence-weighted voting in ensemble models
- ğŸ“ Extend to 3D/multimodal inspection and domain-specific visionâ€“language pretraining

## ğŸ“„ Reference

Based on the original paper:  
[**WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation**](https://arxiv.org/abs/2310.14530)
